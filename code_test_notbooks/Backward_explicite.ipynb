{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ab56e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#On importe les packets nécéssaires pour construire notre modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcda4f0",
   "metadata": {},
   "source": [
    "On crée une couche de convolution invariante par rotation avec son Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb718158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class convZ(torch.autograd.Function):\n",
    "\n",
    "    # On utilise ici des @staticmethods afin de pouvoir choisir comment le Backward est calculé\n",
    "    @staticmethod\n",
    "    # bias est un argument optionnel\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        k=weight.detach().numpy()\n",
    "        Kernel= torch.tensor([[[[k[0][0][0][2],k[0][0][0][1],k[0][0][0][2]], #On construit notre kernel invariant par rotation.\n",
    "                    [k[0][0][0][1],k[0][0][0][0],k[0][0][0][1]],\n",
    "                    [k[0][0][0][2],k[0][0][0][1],k[0][0][0][2]]]]])\n",
    "        ctx.conv1=nn.Conv2d(1,1,3)\n",
    "        ctx.conv1.weight=torch.nn.parameter.Parameter(Kernel) #On configure notre couche de convolution avec\n",
    "                                                              #notre kernel invariant par rotation\n",
    "        output=ctx.conv1(input)\n",
    "        \n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    # Cette fonction n'a qu'une seule sortie, elle ne reçoit donc qu'un seul gradient\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # On utiliser le gradient calculé par l'autograd pour le couches suivantes pour déterminer\n",
    "        # le gradient de cette couche convolutionnelle.\n",
    "        \n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "        k0=torch.tensor([[[[0.,  0., 0.],    #k0,k1,k2 sont les filtres qui correspondent \n",
    "                                             # aux différents coefficients du kernel.\n",
    "                 [ 0.,  1.,  0.],\n",
    "                 [0.,  0., 0.]]]])\n",
    "        k2=torch.tensor([[[[1.,  0., 1.],\n",
    "                 [ 0.,  0.,  0.],\n",
    "                 [1.,  0., 1.]]]])\n",
    "        k1=torch.tensor([[[[0.,  1., 0.],\n",
    "                 [ 1.,  0.,  1.],\n",
    "                 [0.,  1., 0.]]]])\n",
    "        C1=F.conv2d(input,k0)   #On calcule les différents Ci évoqués dans la fiche détails du Backward qui correspondent à \n",
    "                                #des tensors de dérivées partielles de la sortie du réseau de convolution \n",
    "                                #du forward par rapport aux paramètres de notre kernel invriant par rotation.\n",
    "        C2=F.conv2d(input,k1)\n",
    "        C3=F.conv2d(input,k2)\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = None\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            \n",
    "            g=F.conv2d(C1,grad_output) # On calcule les dérivées partielles de la fonction coût par rapport \n",
    "                                       #aux 3 degrés de liberté de notre kernel invariant par rotation.\n",
    "            h=F.conv2d(C2,grad_output)\n",
    "            i=F.conv2d(C3,grad_output)\n",
    "            \n",
    "            grad_weight = torch.cat((g, h, i), 3) # Pour déterminer le gradient final, on concatène les dérivées partielles \n",
    "                                                  #de la fonction coût par rapport aux 3 degrés de \n",
    "                                                  #liberté de notre kernel invariant par rotation.\n",
    "        \n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6664a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConvZ(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyConvZ, self).__init__()\n",
    "        self.fn = convZ.apply\n",
    "        self.weight = nn.Parameter(torch.randn(1, 1, 1, 3)) # On initialise les 3 poids du kernel invariant par \n",
    "                                                            # rotation de notre couche convolutionnelle. \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fn(x, self.weight)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5842c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()  #On crée un petit modèle afin de tester notre Backward.\n",
    "        self.convZ = MyConvZ()\n",
    "        self.pool = nn.AvgPool2d((1, 3))\n",
    "        self.fc1 = nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.convZ(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = x.view(2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e9d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "#On pose construit input1 et labels1 simplement pour que le modèle puisse s'entrainer\n",
    "#et que l'on puisse tester le Backward, sans que cela ait de réelle signification.\n",
    "\n",
    "    input1 = torch.Tensor([\n",
    "                            [[[1, 0, 0, 1,1],\n",
    "                             [0, 1, 0, 1,0],\n",
    "                             [0, 0, 0, 1,1],\n",
    "                             [1, 1, 0, 1,1],\n",
    "                             [1, 0, 1, 1,1]]],\n",
    "                             [[[0, 1, 1, 0,0],\n",
    "                             [1, 0, 1, 0,1],\n",
    "                             [1, 1, 1, 0,0],\n",
    "                             [0, 0, 1, 0,0],\n",
    "                             [0, 1, 0, 0,0]]]\n",
    "                          ])\n",
    "\n",
    "    label1 = torch.LongTensor([0., 1.])\n",
    "\n",
    "    net = Net()\n",
    "\n",
    "    # Définir une fonction de perte et un optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(input1)\n",
    "    loss = criterion(outputs, label1)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_loss = 0.0\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    print(running_loss)\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
